\documentclass[12pt,a4paper,twocolumn]{article}
\usepackage{formelarkJV}
\usepackage{booktabs}
\title{Formidabelark FYS4150}
% Hvordan lage formelark i 4150
% 1. Sjekk tidligere eksamensoppgaver og skriv ned relevante _temaer_
% 2. Finn temaene i slides el.l.
% 3. Skriv ned hovedlinjer/idéer
% 4. Skriv algoritmer/formler
% 5. Rydd opp
% 6. Gjenta
\begin{document}
\maketitle
\tableofcontents
\begin{flushleft} % Unngår like lange linjer -> jevne mellomrom.
Hei
\subsection{Lineær algebra og sånn} % (fold)
\label{sub:linalg}
\textbf{Diskretisering} av $-\frac{d^2u(x)}{dx^2} = f(x,u(x))$. Bruk def. av derivert, det gir. $u^{''}_i \approx  (u_{i+1} -2u_i +u_{i-i})/h^2.$ Kan skrives som tridiagonal matrise
\begin{equation*}
\mathbf{A} = \frac{1}{h^2}\left(\begin{array}{cccccc}
2 & -1 &  &   &  & \\
-1 & 2 & -1 & & & \\
& \dots   & \dots &\dots   &\dots & \\
&   &  &-1  &2& -1 \\
&    &  &   &-1 & 2 \\
\end{array} \right)	
\end{equation*}
Gir likninga $\mathbf{A}\mathbf{u} = \mathbf{f}(\mathbf{u}).$ Som vi kan skrive som $a_iu_{i-1}+b_iu_i+c_iu_{i+1} = f_i,$. Først forover substitusjon,
\begin{verbatim}
	  btemp = b[1];
    u[1] = f[1]/btemp;
    for(i=2 ; i <= n ; i++) {
      temp[i] = c[i-1]/btemp;
      btemp = b[i]-a[i]*temp[i];
      u[i] = (f[i] - a[i]*u[i-1])/btemp;
\end{verbatim}
deretter bakover substitusjon
\begin{verbatim}
	for(i=n-1 ; i >= 1 ; i--) {
      u[i] -= temp[i+1]*u[i+1];
\end{verbatim}
Lurt å plotte feil fra denne metoden som $\epsilon_i = \log(|v_i-u_i|/|u_i|)$, hvor $u_i$ er den analytiske løsninga i punkt $i$.
\textbf{FLOPS} for de forskjellige metodene
\begin{itemize}
	\item Radredusiering $2n^3/n$
	\item LU-dekomp. $2n^3/3$
	\item Tridiagonal løser $8n$ 
	\item QR $4n^3/3$
\end{itemize}
\textbf{LU-dekomponering} Likninga $\mathbf{Ax}=\mathbf{w}$, kan skrives som $\mathbf{A} \mathbf{x} \equiv \mathbf{L} \mathbf{U} \mathbf{x} =\mathbf{w}$. Dette kan regnes i to steg $\mathbf{L} \mathbf{y} = \mathbf{w};\, \mathbf{Ux}=\mathbf{y}$, hvor vi har $\mathbf{y}=\mathbf{Ux}=\mathbf{L^{-1}w}$.\\
\textbf{Jacobis metode} Likninga $\hat{A}\mathbf{x}=\mathbf{b}$ løses ved å bruke $\mathbf{x}^{(k+1)}= \hat{D}^{-1}(\mathbf{b}-(\hat{L}+\hat{U})\mathbf{x}^{(k)})$, hvor $D$ er en diagonal matrise og $L$ og $U$ er nedre og øvre triangulære matriser, $\mathbf{x}^{(k)})$ er et gjett på løsninga. Hvis matrisa $A$ er positiv definitt eller dominant på diagonalen kan man vise at denne metoden alltid vil konvergere. Vi implementerer metoden på denne måten
\begin{itemize}
  \item Definer en toleranse for når ikke-diagonale elemeter er null
  \item Sammenlikn største ikke-diagonale element med toleransen, hvis større enn toleranse må man fortsette å iterere
  \item Velg største ikke-diag. element og regn ut rotasjonsvinkelen etter den
  \item Foreta rotasjonen
  \item Fortsett til maks element i $A\leq\epsilon$
\end{itemize}
\textbf{Egenverdier og `similarity'transformering} Man kan finne egenverdiene av $A$ ved å bruke $\mathbf{B}= \mathbf{S}^T \mathbf{A}\mathbf{S},\, \hspace{1cm}  \mathbf{S}^T\mathbf{S}=\mathbf{S}^{-1}\mathbf{S} =\mathbf{I}$. Gjør man det mange nok ganger ender man opp med $\mathbf{S}_N^T\dots \mathbf{S}_1^T\mathbf{A}\mathbf{S}_1\dots \mathbf{S}_N=\mathbf{D}$, hvor $D$ har egenverdiene til $A$ på diagonalen. Egenverdiene endres ikke av denne transformasjonen: $\mathbf{A}\mathbf{x}=\lambda\mathbf{x} \Rightarrow (\mathbf{S}^T\mathbf{A}\mathbf{S})(\mathbf{S}^T\mathbf{x})=\lambda\mathbf{S}^T\mathbf{x}$, men vi ser at egenvektorene endres! $\to$ må rotere egenvektormatrisa motsatt vei for å få de ``ekte'' egenvektorene (prosjekt 1?).\\
(Jacobis metode for egenverdier) Vi setter
$$ \mathbf{S}=
 \left( 
   \begin{array}{cccccccc}
   1  &    0  & \dots &   0        &    0  & \dots & 0 &   0       \\
   0  &    1  & \dots &   0        &    0  & \dots & 0 &   0       \\
\dots & \dots & \dots & \dots      & \dots & \dots & 0 & \dots     \\ 
   0  &    0  & \dots & \cos\theta  &    0  & \dots & 0 & \sin\theta \\
   0  &    0  & \dots &   0        &    1  & \dots & 0 &   0       \\
\dots & \dots & \dots & \dots      & \dots & \dots & 1 & \dots     \\
   0  &    0  & \dots &  -\sin\theta        &    0  & \dots & 0 &   \cos\theta   
   \end{array}
 \right) $$
 Algoritmisk: 1. Velg toleranse $\epsilon\sim 10^{-8}$. 2. Lag en while-løkke som går så lenge max$(a_{ij}^2)\geq\epsilon$ for $i\neq j$. 3. Bruk det største ikke-diag. elementet $|a_{kl}| =$max$|a_{ij}|$ ($i\neq j$) til å regne ut $\tau=(a_{ll}-a_{kk})/2a_{kl}$, hvor $\tan\theta = -\tau \pm\sqrt{1+\tau^2}$. Velg den løsninga som gir minste rotasjon for å forhindre at andre elementer blir forskjøvet langt fra null. 4. Gjennomfør likhetstransformasjonen $\bf{B}=\bf{S}(k,l,\theta)^T\bf{AB}(k,l,\theta)$, kan gjøres på en lur måte. 5. Fortsett til max$(a_{ij}^2)\leq\epsilon$.\\
 Krever $3n^2-5n^2$ rotasjoner som hver krever $4n$ operasjoner $\to 12n^3-15n^3$ FLOPS. ULEMPE: uvisst hvor mange iterasjoner man trenger. Fröbeniusnormen er alltid bevart ved ortogonale transformasjoner, dette sikrer konvergens for denne metoden.\\
 \textbf{Householders algoritme} er bedre enn Jacobis metode! Starter med $\mathbf{S}=\mathbf{S}_1\mathbf{S}_2\dots\mathbf{S}_{n-2},$ der $S$ er en ortogonal matrise. Man lar den virke på hver side av $A$ og vi ender opp med $$\mathbf{S}^{T} \mathbf{A} \mathbf{S} = 
    \left( \begin{array}{ccccccc} a_{11} & e_1 & 0   & 0    & \dots  &0     & 0 \\
                                e_1 & a'_{22} & e_2 & 0    & \dots  &0     &0 \\
                                0   & e_2 & a''_{33} & e_3  &0       &\dots & 0\\
                                \dots  & \dots & \dots & \dots  &\dots      &\dots & \dots\\
                                0   & \dots & \dots & \dots  &\dots       &a^{(n-1)}_{n-2} & e_{n-1}\\
                                0   & \dots & \dots & \dots  &\dots       &e_{n-1} & a^{(n-1)}_{nn}

             \end{array} \right) .$$
Vi skriver rotasjonsmatrisa som
$$\mathbf{S_{1}} = \left( \begin{array}{cc} 1 & \mathbf{0^{T}} \\
\mathbf{0}& \mathbf{P} \end{array} \right),$$
der $\mathbf{P}=\mathbf{I}-2\mathbf{u}\mathbf{u}^T$. $\mathbf u$ er en vektor vi må finne.
$$\mathbf{S}_1^T\mathbf{A}\mathbf{S}_1 =  \left( \begin{array}{cc} a_{11} & (\mathbf{Pv})^T \\
\mathbf{Pv}& \mathbf{A}' \end{array} \right).$$
$\mathbf v$ er vektoren med elementene i første rad og kolonne i $A$. Vi må også ha at $(\mathbf{Pv})^T = (k,0,\ldots)$. Vi har også bruk for
$$(\mathbf{Pv})^T\mathbf{Pv} = k^{2} = \mathbf{v}^T\mathbf{v}=|v|^2 = \sum_{i=2}^{n}a_{i1}^2,$$
som gir oss at $k=\pm v$.
\begin{equation}
\mathbf{Pv} = \mathbf{v} -2\mathbf{u}( \mathbf{u}^T\mathbf{v})= k \mathbf{e},
\end{equation}
Som vi kan skrive som $\mathbf{v} - k\mathbf{e} = 2\mathbf{u}( \mathbf{u}^T\mathbf{v})$, opphøyer vi dette i andre får vi $2( \mathbf{u}^T\mathbf{v})^2=(v^2\pm a_{21}v)$, (NB: velg fortegnet som gir størst verdi for å unngå numerisk tap) som så settes inn i
$$\mathbf{u}=\frac{\mathbf{v}-k\mathbf{e}}{2( \mathbf{u}^T\mathbf{v})}.$$
Dette settes så inn i $P$ og vi kan rotere ferdig. Denne prosessen gjentas $(n-1)$ ganger for en $n\times n$-matrise. <3\\
\textbf{Lanczos algoritme} brukes på symmetriske egenverdiproblemer. Vi har matrisa $A$. Algoritmen genererer en sekvens med reelle tridiagonale matriser $T_k$ med dim. $k\times k$ ($k\leq n$), s.a. ekstremalegenverdiene til $T_k$ blir bedre og bedre estimater av egenverdiene til $A$. Metoden bruker en likhetstransf. $T = Q^TAQ$.
\textbf{Splineinterpolasjon} er en metode for å interpolere mellom datapunkter. Mest brukt er kubiske spliner, altså polynomer av grad tre som binder sammen datapunktene. Har man et intervall $[x_0,x_n]$ med datapunkter å interpolere så har vi $n$ polynomer av typen $s_i(x) = a_{i0} + a_{i1}x + a_{i2}x^2 + a_{i3}x^3$. Antar at de deriverte er kontinuerlige, s.a. $s'_{i-1}(x_i) = s_i'(x_i)$ og $s''_{i-1}(x_i) = s_i''(x_i)$, dette gir oss $4n$ koeff. å finne og $4n-2$ likn. å løse. Vi setter $s''_i(x_i) \equiv f_i$ og $s''_i(x_{i+1}) \equiv f_{i+1}$, trekker vi en rett linje mellom $f_i$ og $f_{i+1}$ så får vi uttrykket
$$s_i''(x) = \frac{f_i}{x_{i+1}-x_i}(x_{i+1}-x)+\frac{f_{i+1}}{x_{i+1}-x_i}(x-x_i),$$
som kan integreres to ganger, da får vi et tredjeordens polynom hvor vi kan finne integrasjonskonstantene ved å bruke $s_i(x_i)=y_i$ og $s_i(x_{i+1})=y_{i+1}$, uttrykket vi ender opp med kan så skrives om ved å bruke kontinuitetsbetingelsene og vi vil ende opp med en tridiagonal matrise som vi kan løse. Voilà, vi har interpolert.\\
%Lineær algebra og sånn (end)

\subsection{ODE} % (fold)
\label{sub:ode}
Essensiell ting å huske: ``finite difference''-metoder og gaussisk kvadratur hvor steglengde varierer og punkter vektes forskjellig.\\
\textbf{Omskrivning} av ODE. Newtons 2. lov (fjærsystem): $m\ddot x = -kx$. Setter $x(t) \equiv y^{(1)}(t)$ og $v(t) \equiv y^{(2)}(t)$. Det gir oss
$$ m\dot y^{(2)}(t) = - ky^{(1)}(t) \quad \dot y^{(1)}(t)=y^{(2)}(t). $$
\textbf{Eulers metode} Taylorutvikling gir $x_{i+1}=x(t=t_i+h)=x(t_i) + hx'(t_i) + O(h^2),$ hvor $x'(t) = v(t)$. Total feil blir, siden man summerer over alle steg $N = (b-a)/h$,  $NO(h^2) \approx O(h).$ \\
\textbf{Runge-Kutta} (2. orden) Definer $\frac{dy}{dt}=f(t,y),$ $y(t)=\int f(t,y) dt,$ $y_{i+1}=y_i+ \int_{t_i}^{t_{i+1}} f(t,y) dt$. Taylorutvikler $f(t,y)$ om midtpunktet til integrasjonsintervallet, $t_i +h/2$, vi får
$$  \int_{t_i}^{t_{i+1}} f(t,y) dt \approx hf(t_{i+1/2},y_{i+1/2}) +O(h^3) $$
som gir $y_{i+1}=y_i + hf(t_{i+1/2},y_{i+1/2}) +O(h^3)$. Vi vet ikke $y_{i+1/2}$, den er
$$ y_{(i+1/2)}=y_i + \frac{h}{2}\frac{dy}{dt} = y(t_i) + \frac{h}{2}f(t_i,y_i) $$
Vi ender opp med $k_1=hf(t_i,y_i)$ og $k_2=hf(t_{i+1/2},y_i+k_1/2)$ som gir $y_{i+i}\approx y_i + k_2 +O(h^3)$. Man regner m.a.o. et par mellomsteg for å oppnå et bedre estimat. Fjerde ordens RK bruker $k_1=hf(t_i,y_i),\, k_2=hf(t_i+h/2,y_i+k_1/2),\,k_3=hf(t_i+h/2,y_i+k_2/2),\,k_4=hf(t_i+h,y_i+k_3)$ som gir $y_{i+1}=y_i +\frac{1}{6}\left( k_1 +2k_2+2k_3+k_4\right).$ Feil: $O(h^4)$. Geometrisk tolkning: $k_1$ beregner stigninga i $t_i$, $k_2$ beregner stigninga i $t_{i+1/2}$, $k_3$ beregner stigninga i $t_{i+1/2}$ ved hjelp av $k_2$, $k_4$ gjør et anslag på stigninga i $t_{i+1}$.\\
\textbf{Adaptive metoder} steglengden varier ettersom hvor mye funksjonen endrer seg, man sjekker om estimert verdi er innafor en viss toleranse og man avgjør så om steglengden skal øke eller avta i neste iterasjon.
\textbf{Predictor-corrector} Betrakt $dy/dt = f(t,y)$ 1.Regn ut stigninga ved $t_i$, \emph{i.e.} $k_1 = f(t_i,y_i)$. 2. Anslå løsning: $y_{i+1}\approx y(t_i)+hk_1$ (Eulers metode). 3. Bruk anslaget til å regne ut stigninga ved $t_{i+1}$, $k_2=f(t_{i+1},y_{i+1})$. 4. Korriger anslaget for løsning $y_{i+1}\approx y(t_i) + (k_1+k_2)h/2$.\\
\textbf{Løse pendelsystem}
\begin{itemize}
 \item Choose the initial position and speed, with the most common choice \( v(t=0)=0 \) and some fixed value for the position.
 \item Choose the method you wish to employ in solving the problem.
 \item Subdivide the time interval \( [t_i,t_f]  \) into a grid with step size $h=(t_f-t_i)/N, $ where \( N \) is the number of mesh points.
 \item Calculate now the total energy given by $E_0=\frac{1}{2}kx(t=0)^2=\frac{1}{2}k.$
 \item The Runge-Kutta method is used to obtain \( x_{i+1} \) and \( v_{i+1} \) starting from the previous values \( x_i \) and \( v_i \).
 \item When we have computed \( x(v)_{i+1} \) we upgrade  \( t_{i+1}=t_i+h \).
 \item This iterative  process continues till we reach the maximum time \( t_f \).
 \item The results are checked against the exact solution. Furthermore, one has to check the stability of the numerical solution against the chosen number of mesh points \( N \).
\end{itemize}
\textbf{\textsc{Hvordan sjekke feil når man løser ODE}} Man kan sjekke forskjell i energi til systemet mellom numerisk og analytisk løsning, den vil øke ettersom tida går, spesielt ved dårlig oppløsning.
% subsection ode (end)

\subsection{PDE} % (fold)
\label{sub:pde}
\textbf{DIFFUSJONSlikninga 1D} $\nabla^2 u(x,t) =\frac{\partial u(x,t)}{\partial t}$ kan løses på flere måter. Først \textbf{ekpsplisitt} metode, vi bruker forward Euler og den ender opp som en matrisemultiplikasjon. Vi diskretiserer og vår
\begin{equation*}
\frac{u_{i,j+1}-u_{i,j}}{\Delta t}=\frac{u_{i+1,j}-2u_{i,j}+u_{i-1,j}}{\Delta x^2}.
\end{equation*}
som gir oss $u_{i,j+1}= \alpha u_{i-1,j}+(1-2\alpha)u_{i,j}+\alpha u_{i+1,j}$. Som essensielt er $V_{j+1} = \mathbf{A}V_{j}$. Utdrag av kode:
\begin{verbatim}
	u(0) = unew(0) = u(n) = unew(n) = 0.0;
    for (int i = 1; i < n; i++) {
      x =  i*step;
      //  initial condition
      u(i) =  func(x);
      //  intitialise the new vector 
      unew(i) = 0;
    }
   // Time integration
   for (int t = 1; t <= tsteps; t++) {
      for (int i = 1; i < n; i++) {
         // Discretized diff eq
         unew(i) = alpha * u(i-1)
         + (1 - 2*alpha)*u(i) + alpha*u(i+1);
      }
\end{verbatim}
Stabilitetsbetingelse: $\Delta t/\Delta x^2 \le 1/2$ finner vi fra spektralradien $\rho(\mathbf{A}) = \hspace{0.1cm}\mathrm{max}\left\{|\lambda|:\mathrm{det}(\mathbf{A}-\lambda\hat{I})=0\right\}.$ For den \textbf{implisitte} metoden bruker vi backward Euler,
\begin{equation*}
u_t\approx \frac{u(x_i,t_j)-u(x_i,t_j-\Delta t)}{\Delta t},
\end{equation*}
og ender opp med $u_{i,j-1}= -\alpha u_{i-1,j}+(1-2\alpha)u_{i,j}-\alpha u_{i+1,j}$, \emph{i.e.} $\mathbf{A}V_{j} = V_{j-1}$. Denne løses med den tridiagonale løseren vår. Stabil for alle tids- og posisjonssteg. Til slutt har vi \textbf{Crank-Nicolson}
\begin{align*}
  &\frac{\theta}{\Delta x^2}\left(u_{i-1,j}-2u_{i,j}+u_{i+1,j}\right)+\\
  &\frac{1-\theta}{\Delta x^2}\left(u_{i+1,j-1}-2u_{i,j-1}+u_{i-1,j-1}\right)=\\
  &\frac{1}{\Delta t}\left(u_{i,j}-u_{i,j-1}\right),
\end{align*}
velger vi $\theta=1/2$ får vi CN-metoden. Vi ser at vi kan skrive $\left(2\hat{I}+\alpha\hat{B}\right)V_{j}=\left(2\hat{I}-\alpha\hat{B}\right)V_{j-1}$. Altså først gjøre den eksplisitte metoden, deretter bruke resultatet derfra til å løse en tridiagonal likning. NB: Husk at vi må ha $\alpha = (\Delta t + \Delta t/2) / \Delta x^2$.\\
TRUNKERINGSFEIL: verdt å merke seg at det halve tidssteget man bruker i CN gjør at trunkeringsfeilen i tid er forskjellig fra den implisitte metoden. For å finne de bruker vi
\begin{align}
u(x+\Delta x,t)&=u(x,t)+\frac{\partial u(x,t)}{\partial x} \Delta x+\frac{\partial^2 u(x,t)}{2\partial x^2}\Delta x^2\nonumber\\
&+\mathcal{O}(\Delta x^3),\label{eq:taydeltaxpluss} \\
u(x-\Delta x,t)&=u(x,t)-\frac{\partial u(x,t)}{\partial x}\Delta x+\frac{\partial^2 u(x,t)}{2\partial x^2} \Delta x^2\nonumber\\
&+\mathcal{O}(\Delta x^3), \label{eq:taydeltaxminus} \\
u(x,t+\Delta t)&=u(x,t)+\frac{\partial u(x,t)}{\partial t}\Delta t+  \mathcal{O}(\Delta t^2), \label{eq:taydeltatpluss} \\
u(x,t-\Delta t)&=u(x,t)-\frac{\partial u(x,t)}{\partial t}\Delta t+  \mathcal{O}(\Delta t^2), \label{eq:taydeltatminus}
\end{align}
og setter dette inn i de forskjellige metodene. Ta tid og posisjon hver for seg.
\begin{table}[H]
  \centering
  \begin{tabular}{ l l l }
    \toprule
    Metode & Trunkering & Stabilitetskrav \\
    \midrule
  Eksplisitt & $\mathcal{O}(\Delta x^2)$ og $\mathcal{O}(\Delta t)$ & $\Delta t \leq \Delta x^2/2$ \\
  Implisitt & $\mathcal{O}(\Delta x^2)$ og $\mathcal{O}(\Delta t)$ & $\forall \quad\Delta t$ og $\Delta x^2$ \\
  Crank-Nicolson & $\mathcal{O}(\Delta x^2)$ og $\mathcal{O}(\Delta t^2)$ & $\forall \quad\Delta t$ og $\Delta x^2$ \\
    \bottomrule
  \end{tabular}
  \caption{Vi ser her trunkeringsfeil og stabilitetskrav for de tre forskjellige metodene. Vi noterer oss at Crank-Nicolson ser ut til å være den beste metoden.}
  \label{tab:trunkering}
\end{table}
% subsection pde (end)

\subsection{Monte Carlo-metoder} % (fold)
\label{sub:monte_carlo_metoder}
\textbf{Tilfeldige tall} genereres av en funksjon som baserer seg på modulodivisjon, \emph{i.e.} man deler et tall på et annet og svaret er resten. Kalles Linear congruential relations og ser ut som dette $N_i=(aN_{i-1}+c)$MOD$M$, men tallet som returneres er $x_i = N_i/M$ for å sikre at det er mellom 0 og 1. $M$ er perioden til funksjonen og bør være så stort som mulig, $N_0$ er såkornet. $a$ og $c$ velges på en ``lur måte''.
\textbf{Varians} $\sigma^2_X =  \langle x^2 \rangle - \langle x\rangle^2$. Standardavviket går som $\sigma \sim 1/\sqrt N$. Feil for tradisjonelle metoder hvor man int. over en $d$-dim. hyperkube med sider $L$ (inneh. $N = (L/h)^d$int.punkter) går som $N^{-k/d}$. MC er uavh. av dimensjonen til int. SUPERBRA. Vi liker å se på variansen som funksjon av ant. datapunkter ved numerisk integrasjon: $\sigma_N^2 = \sigma_X^2/N$.\\
\textbf{Numerisk integrasjon} Endrer grenser ved å bruke $y=a+(b-a)x$. Vi velger oss en PDF $p(x)$ som vi putter inn i integralet.
$$I=\int_a^b F(x) dx =\int_a^b p(x)\frac{F(x)}{p(x)} dx=\int_{\tilde{a}}^{\tilde{b}}\frac{F(x(y))}{p(x(y))} dy.$$
Hvor vi har $dy/dx=p(x)$ og integralet  kan så skrives
$$ I\approx  \frac{1}{N}\sum_{i=1}^N\frac{F(x(y_i))}{p(x(y_i))},$$
hvor $y_i\in[0,1]$\\
\textbf{Importance sampling} løs $p(y)dy=dx$ for $x$, $x(y) = \int_0^y p(y')dy'$,(husk å løse for $y(x)$) da kan man trekke uniforme tall $x$ og distribuere de etter ønsket PDF, t.d. eksponentialdist.:$p(y)=\exp(-y)$, eller for den uniforme distribusjonen (som vi jo trekker tall fra): $y(x) = a+(b-a)x$, hvor $a$ og $b$ er de opprinnelige integrasjonsgrensene.\\
\textbf{Brute force}-integrering: husk å gange med volumelementet som kommer fra Jacobideterminanten $\Pi_{i=1}^d(b_i-a_i)$, hvor $a_i$ og $b_i$ er integrasjonsgrensene for de forskjellige dimensjonene (i tilfelle flerdim. integral).
\textbf{Akseptering/avvisning} kan brukes i stedet for importance sampling. Man trekker da et tall, sjekker om det er inne i intervallet vi er interessert i og bruker det om det er innafor.
% subsection monte_carlo_metoder (end)

\subsection{Numerisk integrasjon} % (fold)
\label{sub:numerisk_integrasjon}
\textbf{Newton-Cotes} inneh. trapes-, rektangel- og Simpsons metode. Kalles også ``equal step-method.'' Filosofi: diskretisere int.intervall med $N$ punkter for en polynomisk integrand med dim. maks $N-1$. Man tilnærmer integranden med et polynom.\\ 
\textbf{Gaussisk kvadratur} %hvordan finne integrasjonspunkter og vekter
Grunnidé for alle integrasjonsmetoder:
\begin{equation*} 
   I=\int_a^bf(x)dx \approx \sum_{i=1}^N\omega_if(x_i),  
\end{equation*}
GK går ut på å velge en ortogonal basis av polynomer og et sett integrasjonspunkter som vektes forskjellig. Kan skrive integranden $f(x)$ som produkt av vektfunksjonen og en glatt funksjon, $W(x)g(x)$. Integrasjonspkt. er nullpkt. til de valgte ortogonale punktene av grad $N$. Vektene finner vi fra en invers matrise. Vi rep. integranden med et polynom av grad $2N-1$ siden vi har $2N$ likn., $N$ for int.pkt. og $N$ for vektene.\\
\begin{table}[H]
  \centering
  \begin{tabular}{ l l l }
    Vektfunk. & Intervall & Polynom \\
    $W(x)=1$&$x\in[-1,1]$&Legendre\\
    $W(x)=\exp(-x^2)$&$x\in(-\infty,\infty)$&Hermite\\
    $W(x)=x^\alpha\exp(-x)$&$x\in[0,\infty)$&Laguerre\\
    $W(x)=1/\sqrt{1-x^2}$&$x\in[-1,1]$&Chebyshev\\
  \end{tabular}
\end{table}
\textbf{Trapesmetoden} 1. Velg ant. int.pkt. og fikser steglengden, 2. Regn ut $f(a)$ og $f(b)$, og gang disse mde $h/2$. 3. Loop over $n=1\to n-1$, summer opp $f(a+h)+f(a+2h)+\ldots+f(b-h)$. 4. Gang hele summen med $h$ og legg til det du regnet ut i 2.\\
\textbf{Rektangelmetoden} $I=\int_a^bf(x) dx \approx  h\sum_{i=1}^N f(x_{i-1/2})$, m.a.o. man diskretiserer funksjonen i $N$ rektangler, evaluerer funksjonen i midtpunktet i hvert rektangel og ganger med steglengden $h$.\\
\textbf{Simpsons metode} $\int_{-h}^{+h}f(x)dx=\frac{h}{3}\left(f_h + 4f_0 + f_{-h}\right)+O(h^5),$\\
\textbf{Bytte av intervall} $t = (b-a)x/2 + (b+a)/2$, husk å regne ut d$t$.\\
\textbf{Verlet-metoden} finner den ved å Taylorutvikle et kobla likningssett.
\begin{align*} 
x(t+h)&=x(t)+hx^1(t)+(h^2/2)x^2(t)+O(h^3) \\
x_{i+1}&= 2x_i-x_{i-1}+h^2x^{(2)}_i+O(h^4)
\end{align*}
Trunkeringsfeil i fjerde potens siden alle odde ledd kansellerer. Finner hastigheten ved
\begin{align*}
  x^1(t)&= \frac{x_{i+1}-x_{i-1}}{2h} + O(h^2)
\end{align*}
Dette utgjør Verlet. Skriver man om Taylorutviklinga får man LEapfrog, bl.a. ved å bruke halve tidssteg.
\begin{align*}
  x(t+h) &= x(t)+h(x^1(t)+(h/2)x^2(t))+O(h^3) \\
  x(t+h/2) &= (x^1(t)+(h/2)x^2(t))+O(h^2) \\
   x(t+h) &= x(t)+h+x^1(t+h/2)+O(h^3)\\
   \Rightarrow x^1(t+h) &= x^1(t+h/2) + (h/2)x^2(t+h)+O(h^2)
\end{align*}
% subsection numerisk_integrasjon (end)

\subsection{Statistisk fysikk} % (fold)
\label{sub:statistisk_fysikk}
\textbf{Metropolisalgoritmen} for Isingmodellen
\begin{itemize}
  \item Generer startilstand ved å plassere deg ved et tilfeldig spinn
  \item Generer prøvetilstand, \emph{i.e.} snu ett spinn og beregn energidiff. (fem mulige verdier for 2D)
  \item Hvis prøvetilstand har negativ energidiff., så godta den nye tilstanden (energien senkes)
  \item Hvis ikke godtatt: generer tilfeldig tall $r$, sammenlikn med $w=\exp(-\beta\Delta E)$ og godta hvis $r\leq w$
  \item Oppdater forventningsverdier \emph{etc.}
  \item Gjenta til likevekt er nådd, en MC-iterasjon er gjort ved en summasjon over alle spinnene.
\end{itemize}
For å finne når likevekt er nådd så kan man regne ut korrelasjonsfunksjonen for f.eks. magnetiseringa. Hvis det er likevekt så vil kun være fluktuasjoner som gjør at magnetiseringa endrer seg og det er ingen korrelasjon.\\
\textbf{Susceptibilitet m.m.} $\chi = (\mean{M^2} - \mean M^2)/(k_B T)$, varmekap. $C_V = (\mean{E^2} - \mean E^2)/(k_b T^2)$. Kan være lurt å dele disse kvantitetene på antall spinn så man kan sammenligne verdiene for forskjellige gitterstørrelser.
\begin{equation*}
\sigma_E^2=\langle E^2 \rangle-\langle E \rangle^2=
         \frac{1}{Z}\sum_{i=1}^M E_i^2e^{-\beta E_i}-
          \left(\frac{1}{Z}\sum_{i=1}^M E_ie^{-\beta E_i}\right)^2.
\end{equation*}
$\mean E = -\partial \ln Z /\partial \beta$. $C_V = (\partial^2 \ln Z/\partial\beta^2)/(k_b T^2)$.\\
\textbf{Partisjonsfunksjon for 2D-Isingmodell} $Z = 2\exp(8\beta J) + 2\exp(-8\beta J) + 12\exp(-\beta\cdot0) = 4\cosh(8\beta J) + 12$.\\
\textbf{Markovkjeder} har tre viktige egenskaper i dens overgansmatrise: avhenger kun av avstanden mellom to punkter $i-j$ i rommet (homogenitet), isotropisk siden den ikke endres når man går fra $(i,j)$ til $(-i,-j)$, homogen i tid siden den kun avhenger av start- og sluttiden.\\
\textbf{Detaljert balanse} er et krav man innfører for å unngå sykliske løsninger, \emph{i.e.} at den gjentar seg selv, det er $W(j\to i)w_j=W(i\to j)w_j$. Vi kan skrive om dette ved å bruke overgangssannsynligheten $T$ og aksepteringssanns. $A$, $(T_{j\to i}A_{j\to i})/(T_{i\to j}A_{i\to j}) = w_i/w_j$, Bruker vi så Boltzmanndistribusjonen, $w_i = \exp(-\beta E_i)/Z$, så får vi $w_i/w_j = \exp(\beta(E_j-E_i))$. Systemet vårt når en Boltzmanndistribuert likevekt. Hurra.\\
% subsection statistisk_fysikk (end)
\textbf{Spektralradie}
Først eksplisitt forover-Euler, da har vi $\hat A = \mathbf{1} + \alpha \hat B$. Egenverdiene til identitetsmatrisa er 1, mens vi for $\hat B$ bør faktisk regne det ut så vi er helt sikre (selv om vi vet at en \emph{reell} tridiagonal matrise er positiv definitt). Vi kan skrive
$$ b_{i,j} = 2\delta_{i,j} - \delta_{i+1,j} - \delta_{i-1,j}. $$
Egenverdilikninga er gitt som
$$ \hat B \hat v = \lambda \hat v $$
\begin{align*}
  \Rightarrow (\hat B \hat v)_i &= \lambda_i \hat v_i \\
  &= \sum\limits_{j=1}^{n}(2\delta_{i,j} - \delta_{i+1,j} - \delta_{i-1,j})v_j \\
  &= 2v_{i} - v_{i+1} - v_{i-1} = \lambda_i v_i.
\end{align*}
Vi kan så velge en basis, $\sin(\beta\theta)$, å uttrykke egenvektorene i slik at vi får
$$ 2\sin(i\theta) - \sin(i+1\theta) - \sin(i-1\theta) = \lambda_i \sin(i\theta). $$
Bruker vi så identiteten $\sin(x+y) + \sin(x-y) = 2\cos(x)\sin(y)$ så kan vi forenkle uttrykket over til,
\begin{align*}
2(1 - \cos(i\theta) )\sin(i\theta) &= \lambda_i \sin(i\theta) \\
\lambda_i &= 2(1 - \cos(i\theta) ).
\end{align*}
Egenverdiene for $\hat A$ blir da $\Gamma = 1-2\alpha(1 - \cos(i\theta) )$
$$ \Rightarrow -1 < 1-2\alpha(1 - \cos(i\theta) ) < 1 \Rightarrow \alpha < \frac{1}{2}, $$
hvilket impliserer at vi har kravet,
$$ \frac{\Delta t}{\Delta x^2} < \frac{1}{2}, $$
som betyr at vi ikke kan velge tids- og lengdesteg som vi vil og fremdeles få et konvergerende resultat.
\textsc{\large Husk å skrive alt du gjør i oppgaven!}
\end{flushleft}
\end{document}
% \textsc{ALGORITMER ER VIKTIGE}
% \textbf{Oppgave 1} Egenverdier, ``similaritytransformations'', diskretisering av diff.likn., Jacobis metode/algoritme, Householders algoritme.\\
% \textbf{Oppgave 2}PDE+linalg, diffusjonslikn., eksplisitt/implisitt, trunkeringsfeil, tridiagonal løser, FLOPS\\
% \textbf{Oppgave 3} ODE, omskrivning til et sett koblede likn., Eulers algoritme(???), feilestimat, Runge-Kutta, geometrisk tolkning av RK, enhetstesting

% \subsection{Eksamen 2013}
% \textbf{Oppgave 1} ODE, andreordens til to førsteordens koblede likn., Eulers algoritme, Runge-Kutta med feilestimat, geometrisk tolkning av RK.\\
% \textbf{Oppgave 2} PDE, diffusjon, diskretiser, eksplisitt/implisitt, trunkeringsfeil, tridiagonal løser, FLOPS \\
% \textbf{Oppgave 3} Numerisk integrasjon, Gaussisk kvadratur, Legendre-polynomer, Laguerre-pol., MC-metoder, brute force og importance sampling.

% \subsection{Eksamen 2012}
% \textbf{Oppgave 1} Linalg, Gaussisk eliminasjon, LU dekomp., diskretisering, FLOPS, enhetstesting \\
% \textbf{Oppgave 2} Metropolisalgoritmen, antakelser for den, enhetstesting,, markovkjeder,

% \subsection{Eksamen 2011}
% \textbf{Oppgave 1} ODE, omskriving til to koblede, Eulers algoritme, Runge-Kutta, feilestimat \\
% \textbf{Oppgave 2} Numerisk integrasjon, trapesregelen, newton-cotes, gaussisk kvadratur, legendre-polynom, laguerre-polynom, MC-integrasjon, importance sampling.\\
% \textbf{Oppgave 3} Linalg., egenverdier, similarity transforms, diskretisering, jacobis metode, 