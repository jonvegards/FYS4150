\documentclass[norsk, 10pt]{article}
\usepackage{babel}          % Ordelingsregler, osv
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{booktabs}       % Ordentlige tabeller
\usepackage{url}            % Skrive url-er
\usepackage{textcomp}       % Den greske bokstaven micro i text-mode
\usepackage{units}          % Skrive enheter riktig
\usepackage{float}          % Figurer dukker opp der du ber om
\usepackage{lipsum}         % Blindtekst
\usepackage{amsmath, amsfonts, amssymb, amsthm}
\usepackage{caption,subfigure,listings, booktabs}
\usepackage{tikz,graphicx}
\usepackage{sectsty}
\usepackage{cite}

% Setter fonter
\usepackage{bbold,gillius}
\allsectionsfont{\sffamily} % Sans serif på alle overskrifter
%\renewcommand{\abstractname}{Executive Summary}
\captionsetup{width=.8\textwidth, textfont={small,it},labelfont={small,sf}}
\usepackage[sc,osf]{mathpazo} % Palatino


% Kodelisting
\usepackage{verbatim}
\lstset{language=matlab,breaklines=true,numbers=left} % For hele programmer.
%\lstinputlisting[language=matlab]{fil.m}

% Layout
\usepackage[top=1.4in, bottom=1.4in, left=1.7in, right=1.7in]{geometry}
%\usepackage[top=1.2in, bottom=1.7in, left=.7in, right=.7in]{geometry}
\frenchspacing % Rett mellomrom etter punktum.
\linespread{1.1} % Linjeavstand.
\usepackage[colorlinks=true]{hyperref} % Farge på lenker.

% Egendefinerte kommandoer
\newcommand{\dt}{\, {\rm d}t\, }
\newcommand{\dx}{\, {\rm d}x\, }
\newcommand{\dv}{\, {\rm d}v\, }
\newcommand{\dr}{\, {\rm d}r\, }
\newcommand{\dd}{\, {\text d} }
%\newcommand{\dp}{\ {\rm d}p\ }
\newcommand{\R}{\mathbb{R}}
\def\mean#1{\left\langle #1 \right\rangle}
\renewcommand{\exp}{\mathit{e}}
%\DeclareMathOperator{\dt}{dt}
\newcommand{\mb}[1]{\mathbf{#1}}
\def\para#1{\left( #1 \right)}
\newcommand{\ket}[1]{\left|#1\right\rangle}
\newcommand{\bra}[1]{\left\langle#1\right|}
\newcommand{\red}[1]{\textcolor{red}{#1}}


%, trim = 1cm 7cm 1cm 7cm % PDF-filer som bilde

\begin{document}

% Forside
\begin{titlepage}
\begin{center}

\textsf{\Large FYS4150 - Computational Physics\\[0.5cm]
\rule{\linewidth}{0.5mm} \\[0.4cm]
{ \huge \bfseries  PROSJEKT 5}\\[0.10cm]
\rule{\linewidth}{0.5mm} \\[1.5cm]
{\Large Ei litta titt på pre- og postsynaptisk membran}}\\[1.5cm]
\textsc{}\\[1.5cm]

% Av hvem?

\textsf{\begin{minipage}{0.49\textwidth}
    \begin{center} \large
        Kandidat 72
    \end{center}
\end{minipage}}

\vfill

% Dato nederst
\textsf{\large{Dato: \today}}

\end{center}
\end{titlepage}

\abstract{I dette prosjektet løser vi diffusjonslikninga med forover og bakover Euler, Crank-Nicolson og ved Monte Carlo-simulering. Metodene gir alle til en viss grad korrekt svar,  den enkleste metoden å implementere er MC-metoden. Vi studerer også trunkeringsfeil for de forskjellige metodene og ser om de stemmer overens med resultatene, noe de ikke gjør her. En diskusjon av feilene blir også gjort.}

Kode for prosjektet er tilgjengelig  på GitHub-domenet: \url{https://github.com/jonvegards/FYS4150/tree/master/project5}, mens kode for plotting \emph{etc.} er lagt i \url{https://github.com/jonvegards/FYS4150/tree/master/build-project5}

\section*{Introduksjon}
Overgangen mellom to nerveceller kalles en synapse. For at det skal kunne overføres signaler fra den ene cella til den andre så må ``noe'' diffundere gjennom synapsen, dette ``noe'' er signalmolekyler som kalles nevrotransmittere. Disse molekylene kommer i en blobb som vil koble seg på den presynaptiske membranen,  og så slippe molekylene fri slik at de kan diffundere over den synaptiske kløfta og komme fram til den postsynaptiske membranen der de vil bli tatt i mot av reseptorer som bringer signalet videre i cella.

For å modellere dette kan vi bruke diffusjonslikninga i én dimensjon,
$$ D\frac{\partial^2u(x,t)}{\partial x^2} = \frac{\partial u(x,t)}{\partial t}. $$
Hvor vi da har antatt visse ting om systemet vårt: den synaptiske kløfta er jevn og like bred overalt, og at konsentrasjonen av nevrotransmittere varierer kun på langs i bevegelsesretninga.

Vi skal løse denne likninga både analytisk og numerisk slik at vi kan teste programmet på høvelig vis og sammenlikne de ulike metodene. De tre numeriske metodene som testes ut er ekplisitt forover-Euler, implisitt bakover-Euler, og Crank-Nicholson-metoden. Resultatene fra disse metodene vil testes for forskjellige parametre og vi vil finne ut hvilken som gir best resultat.

Systemet beskrevet ovenfor kan også simuleres med Monte Carlo-metoder og tilfeldig gange. Vi vil bruke en algoritme som baserer seg på at nevrotransmitterne kan hoppe fram og tilbake som virrevandrere og så sammenlikne resultatene fra denne metoden med de fra diffusjonslikningssimuleringa.

\section*{Teori}
% Løse systemet analytisk
Å løse systemet analytisk er essensielt for å kunne sjekke at våre numeriske resultater stemmer. Vi setter diffusjonskonstanten $D$ lik 1 slik at vi får en dimensjonsløs likning. Diffusjonslikninga er en separabel likning, slik at vi kan skrive den
\begin{align*}
\mathcal T(t) \frac{\partial^2\mathcal X(x)}{\partial x^2} &= \mathcal X(x) \frac{\partial \mathcal T(t)}{\partial t} \\
\Rightarrow \frac{1}{\mathcal X(x)} \frac{\partial^2\mathcal X(x)}{\partial x^2} = -\lambda^2 &\quad \frac{1}{\mathcal T(t)} \frac{\partial\mathcal T(t)}{\partial t} = -\lambda^2,
\end{align*}
hvor $\lambda$ er en konstant vi snart skal finne. Det viser seg imdlertid at det lønner seg å skrive løsninga som
$$ u(x,t) = \mathcal X(x)\mathcal T(t) + u_s(x), $$
hvor $u_s(x)$ er likevektstilstanden løsninga vår skal konvergere mot. Første leddet i løsninga vil gå mot null ettersom tida øker, så vi står igjen med likevektsleddet som er $u_s(x) = 1-x$. Vi finner så $\mathcal X(x)$,
\begin{align*}
 \frac{\partial^2\mathcal X(x)}{\partial x^2} &= -\lambda^2\mathcal X(x) \\
 \Rightarrow \mathcal X(x) &= A\sin(\lambda x) + B\cos(\lambda x).
\end{align*}
Tidsdelen av løsninga finner vi ved å løse,
\begin{align*}
 \frac{\partial\mathcal T(t)}{\partial t} &= -\lambda^2\mathcal T(t) \\
 \Rightarrow \mathcal T(t) = Ce^{-\lambda^2 t}.
\end{align*}
Dette gir oss
$$ u(x,t) = A\para{\sin\para{\lambda x} + B\cos(\lambda x)}e^{-\lambda^2 t} + 1 - x, $$
Initialbetingelsene er $u(x=d=1,\text{alle } t) = 0$, $u(x=0,t > 0) = u_0 = 1$ som gir oss
$$ u(x,t) = A_n\sin\para{\frac{\pi x}{d}}e^{-\lambda^2 t} + 1 - x. $$
Denne løsninga har mange moduser, slik at vi må skrive den som en sum av alle modene,
$$ u(x,t) = \sum\limits_{n=1}^{\infty}A_n\sin\para{\frac{\pi nx}{d}}e^{-(\pi n)^2 t} + 1 - x.$$
Det gjenstår å finne koeffisientene $A_n$, den finner vi fra initialbetingelsen $u(x,0) = 0$,
$$ 0 = 1 - x + \sum\limits_{n=1}^{\infty} A_n\sin\para{\frac{\pi nx}{d}}, $$
vi ser at dette minner oss om Fourieranalyse og at vi derfor kan finne $A_n$ som en Fourierkoeffisientene til en funksjon $f(x)$,
$$ A_n = \frac{2}{d}\int\limits_0^d f(x)\sin\para{\frac{n\pi x}{d}}\dx. $$
Dette gir oss at $f(x) = x - 1$ og vi får integralet
\begin{align*}
A_n 	&= 2\int\limits_0^1 (x-1)\sin\para{n\pi x}\dx \\
	&= 2\int\limits_0^1x\sin\para{n\pi x}\dx - 2\int\limits_0^1\sin\para{n\pi x}\dx \\
	&= 2\para{ \frac{-\cos(n\pi x)x}{n\pi}\Big|_{x=0}^1 + \int\limits_0^1 \frac{\cos\para{n\pi x}}{n\pi}\dx} - \frac{4}{\pi n}\\
	&= \frac{2}{n\pi} - \frac{4}{\pi n} \\
	&= -\frac{2}{\pi n}.
\end{align*}
Vår endelige analytiske løsning blir da,
$$ u(x,t) = 1 - x - 2\sum\limits_{n=1}^{\infty}\frac{\sin\para{\pi nx}}{\pi n}e^{-(\pi n)^2 t}. $$

% Se på de tre metodene
\paragraph{De tre metodene}
Som allerede nevnt skal vi løse diffusjonslikninga numerisk ved hjelp av tre forskjellige metoder, vi ser først på den eksplisitte forover-Euler-metoden. 
 \[
u_t\approx \frac{u(x_i,t_j+\Delta t)-u(x_i,t_j)}{\Delta t} = \frac{u_{i,j+1}-u_{i,j}}{\Delta t}
\]
og
\[
u_{xx}\approx \frac{u(x_i+\Delta x,t_j)-2u(x_i,t_j)+u(x_i-\Delta x,t_j)}{\Delta x^2} = \frac{u_{i+1,j}-2u_{i,j}+u_{i-1,j}}{h^2},
\]
hvor vi har diskretisert tid og rom, $i,j = 0,1,\ldots$ Vi husker hvordan diffusjonslikninga ser ut og innser at vi kan skrive
\begin{equation} u_{i,j+1} = u_{i,j} + \alpha\para{u_{i+1,j}-2u_{i,j}+u_{i-1,j}},\label{eq:eksplisitt} \end{equation}
hvor $\alpha = \Delta t /h^2$. Vi kan så definere en vektor som inneholder alle funksjonsverdiene til $u$,
$$ V_j^T = [u_{0,j},\ldots,u_{n,j}]. $$
Denne vektoren trenger noen venner, nemlig matriser som sier hvordan vi skal behandle funksjonsverdiene. Vi setter $\hat A \equiv \mathbb 1 + \alpha \hat B$, hvor vi har,
\begin{align}\para{\begin{matrix}
2 & -1 & 0 & \ldots & 0 \\
-1 & 2 & -1 & 0& \vdots \\
0 & \ddots & \ddots & \ddots & -1 \\
\vdots & \ldots & \ldots & -1 & 2 \\
\end{matrix}}.
\end{align}
Vi kan da skrive diffusjonslikninga på matriseform: $\hat V_{j+1} = \hat A \hat V_j$, akkurat som i prosjekt 1. Dette kan vi enkelt implementere i programmet vårt som en dobbel for-løkke som går over tid og rom. Vi vil da ikke definere hele matriser, siden $\hat B$ er tridiagonal så kan vi operere med vektorer og gange riktige elementer sammen slik at vi slipper å sløse FLOPS ved å gange med null mange ganger.

Vi kan så se på implisitt bakover-Euler-metoden. Til forskjell fra den eksplisitte metoden over, så bruker vi her det forrige tidssteget til å finne det nye. Vi har da,
 \[
u_t\approx \frac{u(x_i,t_j)-u(x_i,t_j-\Delta t)}{\Delta t} = \frac{u_{i,j}-u_{i,j-1}}{\Delta t},
\]
mens $u_{xx}$ er den samme som i stad. Dette skriver vi så sammen til å være
\begin{equation}
u_{i,j-1} = u_{i,j} - \alpha\para{u_{i+1,j}-2u_{i,j}+u_{i-1,j}}. \label{eq:implisitt}
\end{equation}
På matriseform får vi da $\hat C = \mathbb 1 - \alpha \hat B$ og dermed $\hat C \hat V_j = V_{j-1} \Rightarrow \hat V_{j+1} = \hat C^{-1}\hat V_{j}$. Vi kan altså her bruke vår egenproduserte tridiagonale regnemaskin fra prosjekt 1.

Siste metode ut er den implisitte Crank-Nicolson-metoden. Vi har et tidssentrert skjema, $(x,t+\Delta t/2)$
 \begin{equation}
u_t\approx \frac{u(x_i,t_j)-u(x_i,t_j-\Delta t)}{\Delta t} = \frac{u_{i,j}-u_{i,j-1}}{\Delta t}.\label{eq:CNtid}
 \end{equation}
Den andreordens deriverte ser nå slik ut,
\begin{align}
u_{xx}&\approx \frac{1}{2}\left(\frac{u(x_i+\Delta x,t_j)-2u(x_i,t_j)+u(x_i-\Delta x,t_j)}{\Delta x^2}+\right.\nonumber\\
&\left. \frac{u(x_i+\Delta x,t_j-\Delta t)-2u(x_i,t_j-\Delta t)+u(x_i-\Delta x,t_j-\Delta t)}{\Delta x^2} \right)\nonumber \\
& = \frac{1}{2}\left(\frac{u_{i+1,j}-2u_{i,j}+u_{i-1,j}}{\Delta x^2}+\frac{u_{i+1,j-1}-2u_{i,j-1}+u_{i-1,j-1}}{\Delta x^2} \right).\label{eq:CNpos}
\end{align}
Vi setter dette så sammen og får,
\begin{align}
& 2u_{i,j}-2u_{i,j-1} = \alpha\para{u_{i+1,j}-2u_{i,j}+u_{i-1,j}}+\alpha\para{u_{i+1,j-1}-2u_{i,j-1}+u_{i-1,j-1}} \nonumber\\
& (2 -2\alpha)u_{i,j-1}+ \alpha u_{i+1,j-1}+\alpha u_{i-1,j-1} = (2+2\alpha) u_{i,j} - \alpha u_{i+1,j}-\alpha u_{i-1,j}, \label{eq:CNmetode}
\end{align}
Vi kjenner igjen høyre- og venstresidene som det vi fikk fra de henholdvis eksplisitte og implisitte Euler-metodene, slik at vi kan skrive
$$ (2\mathbb 1 + \alpha \hat B)\hat V_j = (2\mathbb 1 - \alpha \hat B)\hat V_{j-1},$$
hvor faktoren 2 kommer fra den halve vi ganget $u_{xx}$ med. For å løse dette skriver vi
$$ \hat V_j = (2\mathbb 1 + \alpha \hat B)^{-1}(2\mathbb 1 - \alpha \hat B)\hat V_{j-1}. $$

Vi har nå det meste klart for å implementere disse metodene i programmet vårt. Men før vi gjør det så er det lurt å se litt nærmere på hvilke verdier av  $\Delta t$ og $\Delta x$ som gir oss stabile løsninger. Spektralradien til en matrise er definert som
$$ \rho(\hat{A}) = \hspace{0.1cm}\mathrm{max}\left\{|\lambda|:\mathrm{det}(\hat A-\lambda\mathbb 1)=0\right\}, $$
altså absoluttverdien til den største egenverdien. Dette kan tolkes som radien til den minste sirkelen i $\mathbb C^2$, med senter i origo, som omslutter alle egenverdiene til matrisa $\hat A$. Hvis vi har $\rho(\hat{A}) < 1$, så vil løsningen vår \emph{alltid} konvergere mot en stabil løsning. Hvis matrisa $\hat A $ er positiv definitt så er dette kravet alltid tilfredsstilt. Så for våre tre metoder og de tilhørende tre matriselikninger, så kan vi sjekke om dette kravet er tilfredsstilt.

Først eksplisitt forover-Euler, da har vi $\hat A = \mathbb 1 + \alpha \hat B$. Egenverdiene til identitetsmatrisa er 1, mens vi for $\hat B$ bør faktisk regne det ut så vi er helt sikre (selv om vi vet at en \red{reell} tridiagonal matrise er positiv definitt). Vi kan skrive
$$ b_{i,j} = 2\delta_{i,j} - \delta_{i+1,j} - \delta_{i-1,j}. $$
Egenverdilikninga er gitt som
$$ \hat B \hat v = \lambda \hat v $$
\begin{align*}
	\Rightarrow (\hat B \hat v)_i &= \lambda_i \hat v_i \\
	&= \sum\limits_{j=1}^{n}(2\delta_{i,j} - \delta_{i+1,j} - \delta_{i-1,j})v_j \\
	&= 2v_{i} - v_{i+1} - v_{i-1} = \lambda_i v_i.
\end{align*}
Vi kan så velge en basis, $\sin(\beta\theta)$, å uttrykke egenvektorene i slik at vi får
$$ 2\sin(i\theta) - \sin(i+1\theta) - \sin(i-1\theta) = \lambda_i \sin(i\theta). $$
Bruker vi så identiteten $\sin(x+y) + \sin(x-y) = 2\cos(x)\sin(y)$ så kan vi forenkle uttrykket over til,
\begin{align*}
2(1 - \cos(i\theta) )\sin(i\theta) &= \lambda_i \sin(i\theta) \\
\lambda_i &= 2(1 - \cos(i\theta) ).
\end{align*}
Egenverdiene for $\hat A$ blir da $\Gamma = 1-2\alpha(1 - \cos(i\theta) )$
$$ \Rightarrow -1 < 1-2\alpha(1 - \cos(i\theta) ) < 1 \Rightarrow \alpha < \frac{1}{2}, $$
hvilket impliserer at vi har kravet,
$$ \frac{\Delta t}{\Delta x^2} < \frac{1}{2}, $$
som betyr at vi ikke kan velge tids- og lengdesteg som vi vil og fremdeles få et konvergerende resultat.

Ser vi derimot på det implisitte skjemaet så skal vi finne egenverdiene til $\hat C = \mathbb 1 + \alpha \hat B$, som ved samme utregning som over gir,
$$ \lambda_i = 1 + 2\alpha(1-\cos(i\theta)). $$
Å nei, vi har en egenverdi som alltid er større enn 1! Frykt ei, det implisitte skjemaet finner neste tidssteg ved $ V_j = \hat C^{-1} V_{j-1}$, vi må altså ha at $(1/\lambda_i) < 1$, som er tilfredsstilt her for alle verdier av $\Delta x$ og $\Delta t$. Hurra!

For Crank-Nicolson blir egenverdiene $\gamma = 1-2\alpha(1 - \cos(i\theta) ) / (1 + 2\alpha(1-\cos(i\theta)))$, som gir oss,
\begin{align*}
	1 &> \frac{1-2\alpha(1 - \cos(i\theta) )}{1 + 2\alpha(1-\cos(i\theta))} \\
	1-2\alpha(1 - \cos(i\theta) ) &> 1 + 2\alpha(1-\cos(i\theta)) \\
	0 &< 2\alpha[(1-\cos(i\theta)) + (1+\cos(i\theta))] \\
	&< 4\alpha,
\end{align*}
som åpenbart alltid er tilfredsstilt.

\paragraph{Trunkeringsfeil}
Vi finner trunkeringsfeilen ved å sette inn Taylorutvikle faktorene i uttrykkene som tilnærmer de tids- og posisjonsderiverte for de forskjellige metodene. Vi gjentar først hvordan de tilnærmede deriverte er, deretter Taylorutvikler vi alle de forskjellige leddene og setter så inn i uttrykkene våre.\\
\textbf{Eksplisitt metode}
\begin{equation}
u_t\approx \frac{u(x,t+\Delta t)-u(x,t)}{\Delta t}\label{eq:tideksplisitt}
\end{equation}
\begin{equation}
u_{xx}\approx \frac{u(x+\Delta x,t)-2u(x,t)+u(x-\Delta x,t)}{\Delta x^2}, \label{eq:xeksplisitt}
\end{equation}
\textbf{Implisitt metode}
Som nevnt tidligere er det kun forskjell på den tidsderiverte, vi bruker her det foregående punktet for å finne den tidsderiverte,
\begin{equation}
u_t\approx \frac{u(x,t)-u(x,t-\Delta t)}{\Delta t} \label{eq:implisitt},
\end{equation}
\textbf{Crank-Nicolson} bruker (\ref{eq:CNtid}) og (\ref{eq:CNpos}).

Vi har nå en del ledd å Taylorutvikle, heldigvis kan vi se på \cite{mhj} og finne en del av utviklingene der. Vi ser på utviklingene for de eksplisitte og implisitte metodene først, da utvikler vi om $t+\Delta t$ og $x + \Delta x$.
% Taylor for eksplisitt og implisitt
\begin{align}
u(x+\Delta x,t)&=u(x,t)+\frac{\partial u(x,t)}{\partial x} \Delta x+\frac{\partial^2 u(x,t)}{2\partial x^2}\Delta x^2+\mathcal{O}(\Delta x^3),\label{eq:taydeltaxpluss} \\
u(x-\Delta x,t)&=u(x,t)-\frac{\partial u(x,t)}{\partial x}\Delta x+\frac{\partial^2 u(x,t)}{2\partial x^2} \Delta x^2+\mathcal{O}(\Delta x^3), \label{eq:taydeltaxminus} \\
u(x,t+\Delta t)&=u(x,t)+\frac{\partial u(x,t)}{\partial t}\Delta t+  \mathcal{O}(\Delta t^2), \label{eq:taydeltatpluss} \\
u(x,t-\Delta t)&=u(x,t)-\frac{\partial u(x,t)}{\partial t}\Delta t+  \mathcal{O}(\Delta t^2). \label{eq:taydeltatminus}
\end{align}
Når vi skal utvikle for Crank Nicolson så må vi huske på å ta hensyn til det halve tidssteget, altså å utviklet om $t' = t + \Delta t/2$.
\begin{align}
u(x+\Delta x, t+\Delta t)&=u(x,t')+\frac{\partial u(x,t')}{\partial x}\Delta x+\frac{\partial u(x,t')}{\partial t} \frac{\Delta t}{2} +\frac{\partial^2 u(x,t')}{2\partial x^2}\Delta x^2\nonumber \\ 
&+\frac{\partial^2 u(x,t')}{2\partial t^2}\frac{\Delta t^2}{4} +\frac{\partial^2 u(x,t')}{\partial x\partial t}\frac{\Delta t}{2} \Delta x+ \mathcal{O}(\Delta t^3) \label{eq:deltaxdeltatpluss} \\
%
u(x-\Delta x, t+\Delta t)&=u(x,t')-\frac{\partial u(x,t')}{\partial x}\Delta x+\frac{\partial u(x,t')}{\partial t} \frac{\Delta t}{2} +\frac{\partial^2 u(x,t')}{2\partial x^2}\Delta x^2\\  \nonumber
&+\frac{\partial^2 u(x,t')}{2\partial t^2}\frac{\Delta t^2}{4} -\frac{\partial^2 u(x,t')}{\partial x\partial t}\frac{\Delta t}{2} \Delta x+ \mathcal{O}(\Delta t^3) \label{eq:deltaxmindeltatpluss} \\
%
u(x+\Delta x,t)&=u(x,t')+\frac{\partial u(x,t')}{\partial x}\Delta x-\frac{\partial u(x,t')}{\partial t} \frac{\Delta t}{2} +\frac{\partial^2 u(x,t')}{2\partial x^2}\Delta x^2\\  \nonumber
&+\frac{\partial^2 u(x,t')}{2\partial t^2}\frac{\Delta t^2}{4} -\frac{\partial^2 u(x,t')}{\partial x\partial t}\frac{\Delta t}{2} \Delta x+ \mathcal{O}(\Delta t^3) \label{eq:deltaxplussCN}\\ 
%
u(x-\Delta x,t)&=u(x,t')-\frac{\partial u(x,t')}{\partial x}\Delta x-\frac{\partial u(x,t')}{\partial t} \frac{\Delta t}{2} +\frac{\partial^2 u(x,t')}{2\partial x^2}\Delta x^2 \\  \nonumber
&+\frac{\partial^2 u(x,t')}{2\partial t^2}\frac{\Delta t^2}{4} +\frac{\partial^2 u(x,t')}{\partial x\partial t}\frac{\Delta t}{2} \Delta x+ \mathcal{O}(\Delta t^3) \label{eq:deltaxminCN} \\
%
u(x,t+\Delta t)&=u(x,t')+\frac{\partial u(x,t')}{\partial t}\frac{\Delta t}{2} +\frac{\partial ^2 u(x,t')}{2\partial t^2}\frac{\Delta t^2}{4} + \mathcal{O}(\Delta t^3) \\
%
u(x,t)&=u(x,t')-\frac{\partial u(x,t')}{\partial t}\frac{\Delta t}{2}+\frac{\partial ^2 u(x,t')}{2\partial t^2}\Delta t^2 + \mathcal{O}(\Delta t^3) \label{eq:uxtCN}
\end{align}
Vi er klare til å sette inn! Vi tar den posisjonsderiverte først siden den er lik for eksplisitt og implisitt.
\begin{align}
u_{xx} &\approx \left(u(x,t)+\frac{\partial u(x,t)}{\partial x} \Delta x+\frac{\partial^2 u(x,t)}{2\partial x^2}\Delta x^2+\mathcal{O}(\Delta x^3) - 2 u(x,t)\right. \nonumber\\
& \left.+ u(x,t)-\frac{\partial u(x,t)}{\partial x}\Delta x+\frac{\partial^2 u(x,t)}{2\partial x^2} \Delta x^2+\mathcal{O}(\Delta x^3)\right)\frac{1}{\Delta x^2}\nonumber \\
&=\frac{\partial^2 u(x,t)}{\partial x^2} +\mathcal{O}(\Delta x^2). \label{eq:trunkxx}
\end{align}
Vi fortsetter med den tidsderiverte for eksplisitt metode,
\begin{align}
u_t &\approx \left(u(x,t)+\frac{\partial u(x,t)}{\partial t}\Delta t - u(x,t) +  \mathcal{O}(\Delta t^2) \right)\frac{1}{\Delta t} \nonumber\\
&=\frac{\partial u(x,t)}{\partial t}+ \mathcal{O}(\Delta t). \label{eq:trunkteksplisitt}
\end{align}
For en implisitte metoden får vi
\begin{align}
u_t &\approx \left(u(x,t) - u(x,t) + \frac{\partial u(x,t)}{\partial t}\Delta t  + \mathcal{O}(\Delta t^2) \right)\frac{1}{\Delta t} \nonumber\\
&=\frac{\partial u(x,t)}{\partial t}+ \mathcal{O}(\Delta t). \label{eq:trunktimplisitt} 
\end{align}
Til slutt skal vi sette inn for Crank-Nicolson. Vi setter inn i (\ref{eq:CNtid}),
\begin{align*}
\frac{u_{i,j+1}-u_{i,j}}{\Delta t} &= \left(u(x,t')+\frac{\partial u(x,t')}{\partial t}\frac{\Delta t}{2} +\frac{\partial ^2 u(x,t')}{2\partial t^2}\Delta t^2 - u(x,t')+\frac{\partial u(x,t')}{\partial t}\frac{\Delta t}{2} \right. \\
&\left.-\frac{\partial ^2 u(x,t')}{2\partial t^2}\Delta t^2\right)\frac{1}{\Delta t} + \mathcal{O}(\Delta t^2) \\
&= \frac{\partial u(x,t')}{\partial t}+ \mathcal{O}(\Delta t^2).
\end{align*}

Vi setter så inn i (\ref{eq:CNpos})
\begin{align*}
&\frac{1}{2}\left(\frac{u_{i+1,j}-2u_{i,j}+u_{i-1,j}}{\Delta x^2}+\frac{u_{i+1,j-1}-2u_{i,j-1}+u_{i-1,j-1}}{\Delta x^2} \right) \\
&= \frac 1{2\Delta x^2}\left[u(x,t')+\frac{\partial u(x,t')}{\partial x}\Delta x+\frac{\partial u(x,t')}{\partial t} \frac{\Delta t}{2} +\frac{\partial^2 u(x,t')}{2\partial x^2}\Delta x^2\right.\nonumber \\ 
&+\frac{\partial^2 u(x,t')}{2\partial t^2}\frac{\Delta t^2}{4} +\frac{\partial^2 u(x,t')}{\partial x\partial t}\frac{\Delta t}{2} \Delta x - 2\left(  u(x,t')+\frac{\partial u(x,t')}{\partial t}\frac{\Delta t}{2} +\frac{\partial ^2 u(x,t')}{2\partial t^2}\frac{\Delta t^2}{4} \right) \\
&+ u(x,t')-\frac{\partial u(x,t')}{\partial x}\Delta x+\frac{\partial u(x,t')}{\partial t} \frac{\Delta t}{2} +\frac{\partial^2 u(x,t')}{2\partial x^2}\Delta x^2\\  \nonumber
&+\frac{\partial^2 u(x,t')}{2\partial t^2}\frac{\Delta t^2}{4} -\frac{\partial^2 u(x,t')}{\partial x\partial t}\frac{\Delta t}{2} \Delta x + u(x,t')+\frac{\partial u(x,t')}{\partial x}\Delta x-\frac{\partial u(x,t')}{\partial t} \frac{\Delta t}{2} \\
&+\frac{\partial^2 u(x,t')}{2\partial x^2}\Delta x^2+\frac{\partial^2 u(x,t')}{2\partial t^2}\frac{\Delta t^2}{4} -\frac{\partial^2 u(x,t')}{\partial x\partial t}\frac{\Delta t}{2} \Delta x -2\left(u(x,t')-\frac{\partial u(x,t')}{\partial t}\frac{\Delta t}{2}\right.\\
&\left.+\frac{\partial ^2 u(x,t')}{2\partial t^2}\frac{\Delta t^2}{4} \right)+u(x,t')-\frac{\partial u(x,t')}{\partial x}\Delta x-\frac{\partial u(x,t')}{\partial t} \frac{\Delta t}{2} +\frac{\partial^2 u(x,t')}{2\partial x^2}\Delta x^2 \\  \nonumber
&\left.+\frac{\partial^2 u(x,t')}{2\partial t^2}\frac{\Delta t^2}{4} +\frac{\partial^2 u(x,t')}{\partial x\partial t}\frac{\Delta t}{2} \Delta x\right] + \mathcal O(\Delta x^2) \\
&= \frac{\partial^2 u(x,t')}{2\partial x^2} + \mathcal O(\Delta x^2).
\end{align*}
Oppsummert kan vi sette resultatene våre i tabell (\ref{tab:trunkering})
\begin{table}[H]
  \centering
  \begin{tabular}{ l l l }
    \toprule
    Metode & Trunkering & Stabilitetskrav \\
    \midrule
	Eksplisitt & $\mathcal{O}(\Delta x^2)$ og $\mathcal{O}(\Delta t)$ & $\Delta t \leq \Delta x^2/2$ \\
	Implisitt & $\mathcal{O}(\Delta x^2)$ og $\mathcal{O}(\Delta t)$ & $\forall \quad\Delta t$ og $\Delta x^2$ \\
	Crank-Nicolson & $\mathcal{O}(\Delta x^2)$ og $\mathcal{O}(\Delta t^2)$ & $\forall \quad\Delta t$ og $\Delta x^2$ \\
    \bottomrule
  \end{tabular}
  \caption{Vi ser her trunkeringsfeil og stabilitetskrav for de tre forskjellige metodene. Vi noterer oss at Crank-Nicolson ser ut til å være den beste metoden.}
  \label{tab:trunkering}
\end{table}

% Monte Carlo-metoder

\section*{Metode}
% Skrive ned algoritmer vi bruker
\paragraph{Eksplisitt, implisitt og Crank-Nicolson}
I (\ref{eq:eksplisitt}) og (\ref{eq:implisitt}) så har vi i grunnen alt vi trenger for å kunne implementere alle tre metodene. Den eksplisitte metoden er en regn matrisemultiplikasjon som trenger én for-løkke, mens for den implisitte metoden kan vi bruke den tridiagonale løseren vi brukte i prosjekt 1, vi har dog reimplementert den for å luke ut insekter som satt i fra det prosjektet. Crank-Nicolson-metoden bruker de to foregående metodene i kombinasjon med halve tidssteg. Rent algoritmisk kan vi skrive det som følger.
\begin{itemize}
\item
Generer starttilstand etc.
\item
Regn ut matrisemultiplikasjonen med den eksplisitte metoden.
\item
Bruk resultatet fra forrige punkt til å sende inn i den tridiagonale løseren.
\item
Gjenta for så mange tidssteg du vil.
\end{itemize}

\paragraph{Monte Carlo-metoden}
Vi kan også simulere diffusjon ved hjelp av Monte Carlo-simuleringer. Prinsippet er ganske enkelt, vi lar hver partikkel som skal over den synaptiske kløfta være representert av en virrevandrer og sjekker hvor langt den kommer på et visst antall tidssteg. I dette prosjektet skal vi følge \cite{farnell} ved å bruke en steglengde på $\sqrt{2D\Delta t}$, hvor $D$ er diffusjonskonstanten som vi har satt til å være 1. Vi skal også sjekke om det har noe å si om vi endrer steglengden til å være gaussisk fordelt om 1 med standardavvik 0.

Algoritmen vår vil basere seg på en virrevandring, slik som i \cite{mhj},
\begin{itemize}
\item 
Generer initialbetingelser
\item 
Løkke over tid og vandrere
\item
Gjør en MC-test for å bestemme om vandreren skal gå fram- eller bakover
\item
Oppdater posisjonsverdier
\item
Gjenta for alle partikler for et antall tidssteg
\item
Regn ut sannsynlighetsdistribusjonen for hvor ofte hver posisjon mellom synapsekantene ble besøkt av en vandrer
\end{itemize}

Siden vi har diskretisert intervallet mellom 0 og 1 i 100 båser, så må vi sjekke om partikkelen klarer å hoppe langt nok når vi har en steglengde som varierer. Dette løser vi ved å sette inn noen flere if-tester i den indre for-løkka, ellers så er algoritmen den samme.

\section*{Resultat og diskusjon}
Vi kan endelig se på resultatene! Overraskende nok ser vi i fig. (\ref{fig:analnum}) at Crank-Nicolson ikke er den beste metoden i dette tilfellet. Ut i fra tab. (\ref{tab:trunkering}) så burde CN være den beste metoden. Grunner for at vi ikke fikk det samme resultatet her kan være at vårt valg av tidssteg for Crank-Nicolson ikke stemmer overens med måten vi har implementert algoritmen på, siden kurven legger seg over den analytiske løsningen kan det tenkes at tidsstegene er for lange. Men det er også pussig at den implisitte metoden legger seg så nøyaktig oppå den analytiske løsninga. En annen feilkilde kan være at tidsstegene mellom den analytiske og de numeriske løsningene ikke stemmer helt overens siden vi har brukt Python til å plotte den analytiske løsninga, da kan det ha blitt en liten forskjell underveis. Men igjen, siden CN-metoden legger seg så godt inntil den eksplisitte metoden er det grunn til å tro at det er tidsstegene som er litt for store.
% Sammenlikne for forskjellige t-verdier
\begin{figure}[H]
\centerline{\includegraphics[scale = 0.5]{oppgave_d.eps}}
\caption{Det ser ut til at den implisitte metoden fungerer best. Det er litt overraskende, siden Crank-Nicolson burde ``tatt det beste'' fra to verdener, i tillegg til at den har et tidssentrert skjema. En mulig feil her kan være at tidsstegene brukt i den analytiske løsninga og den numeriske ikke stemmer overens.}
\label{fig:analnum}
\end{figure}
Videre har vi den MC-simulerte løsninga. Her har vi foretatt en virrevandring med $10^7$ partikler over like lang tid som for de andre metodene. Vi ser i fig. (\ref{fig:analMC}) de to MC-metodene plottet sammen med den analytiske løsninga. Det første vi noterer oss er at når steglengdene er gaussiske så må vi la simuleringa gå for en lenger tid siden partiklene ikke alltid klarer å hoppe langt nok for å komme i neste bås. Det andre vi noterer oss er at den numeriske løsninga legger seg pent oppå den analytiske løsninga, igjen kan vi tenke på fig. (\ref{fig:analnum}) og vurdere hvorvidt steglengden til CN-metoden er for lang eller ei, siden MC-løsninga legger seg så fint oppå den analytiske kan vi nok konkludere med at CN-metoden går for litt langt.

Men siden vi så at MC-simuleringa må kjøre lenger ved en gaussisk varierende steglengde kan vi også prøve å ta hensyn til at den varierer, \emph{i.e.} akkumelere opp hoppene, slik at to hopp kan være nok til at vi kan oppdatere posisjonsvektorene  i programmet.
\begin{figure}[H]
\centerline{\includegraphics[scale = 0.5]{MC_walk.eps}}
\caption{Monte Carlo-simulering der steglengden er fast og gaussisk varierende. Vi ser at siden partiklene ikke alltid klarer å hoppe langt nok at simuleringa må kjøre lenger for å oppnå samme resultat som ved fast steglengde.}
\label{fig:analMC}
\end{figure}
Som en siste figur viser vi hva som skjer vi setter $\Delta t = 3h^2/2$, som ikke tilfredsstiller stabilitetskravet til den eksplisitte metoden.
\begin{figure}[H]
\centerline{\includegraphics[scale = 0.5]{oppgave_d_alfa.eps}}
\caption{Vi ser at stabilitetskravet ikke er noe humbug! Når $\Delta t = 3h^2/2$ så divergerer løsninga til ad undas.}
\label{fig:alfa}
\end{figure}
\subsection*{Konklusjon}
I dette prosjektet så har vi sett på fire måter å løse samme problem på. Vi har sett at Crank-Nicolson var en litt skuffende metode her, men hadde vi hatt mer tid til rådighet kunne vi dykket dypere i koden og prøvd å rette opp feilen. Vår konklusjon på at den ikke fungerte tilfredsstillende er at steglengden ikke stemmer helt. Videre simulerte vi systemet med Monte Carlo-metoder, det fungerte utmerket med en fast steglengde. En feil som kan ha blitt gjort i dette programmet er at steglengden vi brukte kanskje ikke stemte helt overens med det \cite{farnell} brukte siden vi skar gjennom og delte opp den synaptiske kløfta i 100 båser, men antakeligvis ville ikke det hatt mye å si.

Til slutt kan vi nevne at når steglengden varierer gaussisk så burde vi tatt hensyn til at to små hopp kan være nok til at partikkelen kommer seg til neste bås, da ville denne metoden antakeligvis vært ganske nærme den med fast steglengde. Når vi ikke tar hensyn til det slik vi har det i denne rapporten, så vil det naturligvis ta lenger tid for systemet å nå likevekt når ikke hvert forsøk på å hoppe gjør at partikkelen hopper fram eller tilbake.

\subsubsection*{Kommentarer til prosjektet}
Dette prosjektet kom på et ubeleilig tidspunkt og det var derfor ikke tid til å rette opp feilene som har blitt kommentert underveis i rapporten. Utenom det så har det vært et bra prosjekt.

\begin{thebibliography}{9}
%\cite{farnell}
\bibitem{farnell}
L.~Farnell,~W.G.~Gibson,
``Monte Carlo simulation of diffusion in a spatially nonhomogeneous medium: A biased random walk on an asymmetrical lattice,''
Journ.\ of Comp.\ Phys.,\ {\bf208}, (2005), 253-265, ISSN 0021-9991, \url{http://www.sciencedirect.com/science/article/pii/S0021999105001087}

%\cite{mhj}
\bibitem{mhj}
M.~H.~Jensen,
``Random walks, Brownian motion and the Metropolis algorithm,''
\url{http://compphysics.github.io/ComputationalPhysics1/doc/pub/rw/html/rw.html}, 

%\cite{mhj}
\bibitem{mhjpde}
M.~H.~Jensen,
``Partial differential equations,''
\url{http://compphysics.github.io/ComputationalPhysics1/doc/pub/pde/html/pde.html}, 


\end{thebibliography}  

\end{document}